{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tqdm\n",
    "import string \n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.calibration import CalibratedClassifierCV \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re \n",
    "\n",
    "import nltk # for stopwords \n",
    "from nltk.corpus import stopwords\n",
    "import gensim # for Word2Vec embeddings \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from konlpy.tag import Mecab, Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('news_train.csv')\n",
    "test = pd.read_csv('news_test.csv') \n",
    "submission = pd.read_csv('sample_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>ord</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>[이데일리 MARKETPOINT]15:32 현재 코스닥 기관 678억 순매도</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>\"실적기반\" 저가에 매집해야 할 8월 급등유망주 TOP 5 전격공개</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>하이스탁론, 선취수수료 없는 월 0.4% 최저금리 상품 출시</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>종합 경제정보 미디어 이데일리 - 무단전재 &amp; 재배포 금지</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWS09727</td>\n",
       "      <td>20200626</td>\n",
       "      <td>롯데·공영 등 7개 TV 홈쇼핑들, 동행세일 동참</td>\n",
       "      <td>전국적인 소비 붐 조성에 기여할 예정</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_id      date                        title  \\\n",
       "0  NEWS02580  20200605          [마감]코스닥 기관 678억 순매도   \n",
       "1  NEWS02580  20200605          [마감]코스닥 기관 678억 순매도   \n",
       "2  NEWS02580  20200605          [마감]코스닥 기관 678억 순매도   \n",
       "3  NEWS02580  20200605          [마감]코스닥 기관 678억 순매도   \n",
       "4  NEWS09727  20200626  롯데·공영 등 7개 TV 홈쇼핑들, 동행세일 동참   \n",
       "\n",
       "                                      content  ord  info  \n",
       "0  [이데일리 MARKETPOINT]15:32 현재 코스닥 기관 678억 순매도    1     0  \n",
       "1       \"실적기반\" 저가에 매집해야 할 8월 급등유망주 TOP 5 전격공개    2     1  \n",
       "2           하이스탁론, 선취수수료 없는 월 0.4% 최저금리 상품 출시    3     1  \n",
       "3            종합 경제정보 미디어 이데일리 - 무단전재 & 재배포 금지    4     0  \n",
       "4                        전국적인 소비 붐 조성에 기여할 예정    1     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>ord</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWS00237</td>\n",
       "      <td>20200118</td>\n",
       "      <td>[주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대</td>\n",
       "      <td>마이크로 LED TV 장비 양산 돌입- 전방업체 투자 확대로 본업도 호조연일 '신고가'</td>\n",
       "      <td>1</td>\n",
       "      <td>NEWS00237_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEWS00237</td>\n",
       "      <td>20200118</td>\n",
       "      <td>[주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대</td>\n",
       "      <td>[이데일리 김대웅 기자] 반도체 장비 업체 코세스(089890)의 기술력이 마이크로...</td>\n",
       "      <td>2</td>\n",
       "      <td>NEWS00237_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWS00237</td>\n",
       "      <td>20200118</td>\n",
       "      <td>[주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대</td>\n",
       "      <td>최근 대형 업체들과 거래를 맺고 관련 장비들의 양산에 돌입하면서 주가도 연일 신고가...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEWS00237_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWS00237</td>\n",
       "      <td>20200118</td>\n",
       "      <td>[주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대</td>\n",
       "      <td>TV를 필두로 올해부터 마이크로 LED의 시대가 본격적으로 개화할 것으로 예상되면서...</td>\n",
       "      <td>4</td>\n",
       "      <td>NEWS00237_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWS00237</td>\n",
       "      <td>20200118</td>\n",
       "      <td>[주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대</td>\n",
       "      <td>코세스는 반도체 장비를 제조, 판매하는 업체로 지난 2006년 11월 코스닥 시장에...</td>\n",
       "      <td>5</td>\n",
       "      <td>NEWS00237_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_id      date                                title  \\\n",
       "0  NEWS00237  20200118  [주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대   \n",
       "1  NEWS00237  20200118  [주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대   \n",
       "2  NEWS00237  20200118  [주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대   \n",
       "3  NEWS00237  20200118  [주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대   \n",
       "4  NEWS00237  20200118  [주목!e스몰캡]코세스, 마이크로LED 시장 개화 최대수혜 기대   \n",
       "\n",
       "                                             content  ord           id  \n",
       "0   마이크로 LED TV 장비 양산 돌입- 전방업체 투자 확대로 본업도 호조연일 '신고가'    1  NEWS00237_1  \n",
       "1  [이데일리 김대웅 기자] 반도체 장비 업체 코세스(089890)의 기술력이 마이크로...    2  NEWS00237_2  \n",
       "2  최근 대형 업체들과 거래를 맺고 관련 장비들의 양산에 돌입하면서 주가도 연일 신고가...    3  NEWS00237_3  \n",
       "3  TV를 필두로 올해부터 마이크로 LED의 시대가 본격적으로 개화할 것으로 예상되면서...    4  NEWS00237_4  \n",
       "4  코세스는 반도체 장비를 제조, 판매하는 업체로 지난 2006년 11월 코스닥 시장에...    5  NEWS00237_5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_tokenized'] = train['title'].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content_tokenized'] = train['content'].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['title_tokenized'] = test['title'].apply(lambda x: mecab.morphs(x))\n",
    "test['content_tokenized'] = test['content'].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in title\n",
    "train['num_words_title'] = train['title_tokenized'].apply(lambda x: len(x)) \n",
    "test['num_words_title'] = test['title_tokenized'].apply(lambda x: len(x))   \n",
    "\n",
    "# number of words in content \n",
    "train['num_words_content'] = train['content_tokenized'].apply(lambda x: len(x)) \n",
    "test['num_words_content'] = test['content_tokenized'].apply(lambda x: len(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words in title \n",
    "train['title_num_unique'] = train['title_tokenized'].apply(lambda x: len(set(x))) \n",
    "test['title_num_unique'] = test['title_tokenized'].apply(lambda x: len(set(x))) \n",
    "\n",
    "# number of unique words in content \n",
    "train['content_num_unique'] = train['content_tokenized'].apply(lambda x: len(set(x))) \n",
    "test['content_num_unique'] = test['content_tokenized'].apply(lambda x: len(set(x))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of punctuations in text \n",
    "train['num_punctuations_title'] = train['title_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "test['num_punctuations_title'] = test['title_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "\n",
    "train['num_punctuations_content'] = train['content_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "test['num_punctuations_content'] = test['content_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 또는 (로 시작하는지에 대한 여부 \n",
    "train[\"title_startswith_[\"]=train.title.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\")) \n",
    "train[\"content_startswith_[\"]=train.content.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\") ) \n",
    "test[\"title_startswith_[\"]=test.title.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\")) \n",
    "test[\"content_startswith_[\"]=test.content.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\") ) \n",
    "\n",
    "\n",
    "# ] 또는 )로 시작하는지에 대한 여부 \n",
    "train[\"title_endswith_]\"]=train.title.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\"))\n",
    "train[\"content_endswith_]\"]=train.content.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\") )\n",
    "test[\"title_endswith_]\"]=test.title.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\"))\n",
    "test[\"content_endswith_]\"]=test.content.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\") )\n",
    "\n",
    "\n",
    "# ' 로 시작하는지에 대한 여부 \n",
    "train[\"title_startswith_quote\"]=train.title.apply(lambda x : str(x).startswith('\"'))\n",
    "train[\"content_startswith_quote\"]=train.content.apply(lambda x : str(x).startswith('\"'))\n",
    "test[\"title_startswith_quote\"]=test.title.apply(lambda x : str(x).startswith('\"'))\n",
    "test[\"content_startswith_quote\"]=test.content.apply(lambda x : str(x).startswith('\"'))\n",
    "\n",
    "\n",
    "# '로 끝나는지에 대한 여부\n",
    "train[\"title_endswith_quote\"]=train.title.apply(lambda x : str(x).endswith('\"'))\n",
    "train[\"content_endswith_quote\"]=train.content.apply(lambda x : str(x).endswith('\"'))\n",
    "test[\"title_endswith_quote\"]=test.title.apply(lambda x : str(x).endswith('\"'))\n",
    "test[\"content_endswith_quote\"]=test.content.apply(lambda x : str(x).endswith('\"'))\n",
    "\n",
    "\n",
    "# 숫자로 시작하는지에 대한 여부 \n",
    "train[\"title_startswith_number\"]=train.title.apply(lambda x : str(x)[0].isdigit())\n",
    "train[\"content_startswith_number\"]=train.content.apply(lambda x : str(x)[0].isdigit())\n",
    "test[\"title_startswith_number\"]=test.title.apply(lambda x : str(x)[0].isdigit())\n",
    "test[\"content_startswith_number\"]=test.content.apply(lambda x : str(x)[0].isdigit())\n",
    "\n",
    "\n",
    "# 숫자로 끝나는지에 대한 여부 \n",
    "train[\"title_endswith_number\"]=train.title.apply(lambda x : str(x)[-1].isdigit())\n",
    "train[\"content_endswith_number\"]=train.content.apply(lambda x : str(x)[-1].isdigit())\n",
    "test[\"title_endswith_number\"]=test.title.apply(lambda x : str(x)[-1].isdigit())\n",
    "test[\"content_endswith_number\"]=test.content.apply(lambda x : str(x)[-1].isdigit())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title length \n",
    "train[\"title_length\"] = train['title'].apply(lambda x : len(x))\n",
    "test[\"title_length\"] = test['title'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content length \n",
    "train['content_length'] = train['content'].apply(lambda x: len(x))\n",
    "test['content_length'] = test['content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average length of word in title \n",
    "train['title_mean_length'] = train['title_tokenized'].apply(lambda x: np.mean([len(w) for w in x])) \n",
    "test['title_mean_length'] = test['title_tokenized'].apply(lambda x: np.mean([len(w) for w in x])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 29), (142565, 29))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_tokenizer(text):  \n",
    "    tokens_mecab = mecab.morphs(text) \n",
    "    return tokens_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for singular value decomposition  \n",
    "tfidf_vec = TfidfVectorizer(tokenizer = mecab_tokenizer, ngram_range = (1,3)) \n",
    "full_tfidf_title = tfidf_vec.fit_transform(train['title'].values.tolist())\n",
    "train_tfidf_title = tfidf_vec.transform(train['title'].values.tolist()) \n",
    "test_tfidf_title = tfidf_vec.transform(test['title'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 81195), (118745, 81195), (142565, 81195))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tfidf_title.shape, train_tfidf_title.shape, test_tfidf_title.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for singular value decomposition  \n",
    "tfidf_vec = TfidfVectorizer(tokenizer = mecab_tokenizer, ngram_range = (1,3)) \n",
    "full_tfidf_content = tfidf_vec.fit_transform(train['content'].values.tolist()) \n",
    "train_tfidf_content = tfidf_vec.transform(train['content'].values.tolist()) \n",
    "test_tfidf_content = tfidf_vec.transform(test['content'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 1237825), (118745, 1237825), (142565, 1237825))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tfidf_content.shape, train_tfidf_content.shape, test_tfidf_content.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_obj = TruncatedSVD(n_components = 20, algorithm = 'arpack') \n",
    "svd_obj.fit(full_tfidf_title)\n",
    "train_svd_title = pd.DataFrame(svd_obj.transform(train_tfidf_title)) \n",
    "test_svd_title = pd.DataFrame(svd_obj.transform(test_tfidf_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svd_title.columns = ['svd_word_title_' + str(i) for i in range(20)] \n",
    "test_svd_title.columns = ['svd_word_title_' + str(i) for i in range(20)] \n",
    "\n",
    "train = pd.concat([train, train_svd_title], axis = 1) \n",
    "test = pd.concat([test, test_svd_title], axis = 1)  \n",
    "\n",
    "del full_tfidf_title, train_tfidf_title, test_tfidf_title, train_svd_title, test_svd_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_obj = TruncatedSVD(n_components = 20, algorithm = 'arpack') \n",
    "svd_obj.fit(full_tfidf_content) \n",
    "train_svd_content = pd.DataFrame(svd_obj.transform(train_tfidf_content)) \n",
    "test_svd_content = pd.DataFrame(svd_obj.transform(test_tfidf_content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svd_content.columns = ['svd_word_content_' + str(i) for i in range(20)] \n",
    "test_svd_content.columns = ['svd_word_content_' + str(i) for i in range(20)] \n",
    "\n",
    "train = pd.concat([train, train_svd_content], axis = 1) \n",
    "test = pd.concat([test, test_svd_content], axis = 1)  \n",
    "\n",
    "del full_tfidf_content, train_tfidf_content, test_tfidf_content, train_svd_content, test_svd_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 69), (142565, 69))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_no_data_leak.csv',index=False) \n",
    "test.to_csv('test_no_data_leak.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
