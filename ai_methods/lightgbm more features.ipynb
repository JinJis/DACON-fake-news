{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tqdm\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('news_train.csv')\n",
    "test = pd.read_csv('news_test.csv') \n",
    "submission = pd.read_csv('sample_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.calibration import CalibratedClassifierCV \n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re \n",
    "\n",
    "import nltk # for stopwords \n",
    "from nltk.corpus import stopwords\n",
    "import gensim # for Word2Vec embeddings \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from konlpy.tag import Mecab, Hannanum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Meta Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_tokenized'] = train['title'].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content_tokenized'] = train['content'].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['title_tokenized'] = test['title'].apply(lambda x: mecab.morphs(x))\n",
    "test['content_tokenized'] = test['content'].apply(lambda x: mecab.morphs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in title\n",
    "train['num_words_title'] = train['title_tokenized'].apply(lambda x: len(x)) \n",
    "test['num_words_title'] = test['title_tokenized'].apply(lambda x: len(x))   \n",
    "\n",
    "# number of words in content \n",
    "train['num_words_content'] = train['content_tokenized'].apply(lambda x: len(x)) \n",
    "test['num_words_content'] = test['content_tokenized'].apply(lambda x: len(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique words in title \n",
    "train['title_num_unique'] = train['title_tokenized'].apply(lambda x: len(set(x))) \n",
    "test['title_num_unique'] = test['title_tokenized'].apply(lambda x: len(set(x))) \n",
    "\n",
    "# number of unique words in content \n",
    "train['content_num_unique'] = train['content_tokenized'].apply(lambda x: len(set(x))) \n",
    "test['content_num_unique'] = test['content_tokenized'].apply(lambda x: len(set(x))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of punctuations in text \n",
    "train['num_punctuations_title'] = train['title_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "test['num_punctuations_title'] = test['title_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "\n",
    "train['num_punctuations_content'] = train['content_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))\n",
    "test['num_punctuations_content'] = test['content_tokenized'].apply(lambda x: len([c for c in x if c in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ 또는 (로 시작하는지에 대한 여부 \n",
    "train[\"title_startswith_[\"]=train.title.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\")) \n",
    "train[\"content_startswith_[\"]=train.content.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\") ) \n",
    "test[\"title_startswith_[\"]=test.title.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\")) \n",
    "test[\"content_startswith_[\"]=test.content.apply(lambda x : str(x).startswith(\"[\" ) or str(x).startswith(\"(\") ) \n",
    "\n",
    "\n",
    "# ] 또는 )로 시작하는지에 대한 여부 \n",
    "train[\"title_endswith_]\"]=train.title.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\"))\n",
    "train[\"content_endswith_]\"]=train.content.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\") )\n",
    "test[\"title_endswith_]\"]=test.title.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\"))\n",
    "test[\"content_endswith_]\"]=test.content.apply(lambda x : str(x).endswith(\"]\" ) or str(x).endswith(\")\") )\n",
    "\n",
    "\n",
    "# ' 로 시작하는지에 대한 여부 \n",
    "train[\"title_startswith_quote\"]=train.title.apply(lambda x : str(x).startswith('\"'))\n",
    "train[\"content_startswith_quote\"]=train.content.apply(lambda x : str(x).startswith('\"'))\n",
    "test[\"title_startswith_quote\"]=test.title.apply(lambda x : str(x).startswith('\"'))\n",
    "test[\"content_startswith_quote\"]=test.content.apply(lambda x : str(x).startswith('\"'))\n",
    "\n",
    "\n",
    "# '로 끝나는지에 대한 여부\n",
    "train[\"title_endswith_quote\"]=train.title.apply(lambda x : str(x).endswith('\"'))\n",
    "train[\"content_endswith_quote\"]=train.content.apply(lambda x : str(x).endswith('\"'))\n",
    "test[\"title_endswith_quote\"]=test.title.apply(lambda x : str(x).endswith('\"'))\n",
    "test[\"content_endswith_quote\"]=test.content.apply(lambda x : str(x).endswith('\"'))\n",
    "\n",
    "\n",
    "# 숫자로 시작하는지에 대한 여부 \n",
    "train[\"title_startswith_number\"]=train.title.apply(lambda x : str(x)[0].isdigit())\n",
    "train[\"content_startswith_number\"]=train.content.apply(lambda x : str(x)[0].isdigit())\n",
    "test[\"title_startswith_number\"]=test.title.apply(lambda x : str(x)[0].isdigit())\n",
    "test[\"content_startswith_number\"]=test.content.apply(lambda x : str(x)[0].isdigit())\n",
    "\n",
    "\n",
    "# 숫자로 끝나는지에 대한 여부 \n",
    "train[\"title_endswith_number\"]=train.title.apply(lambda x : str(x)[-1].isdigit())\n",
    "train[\"content_endswith_number\"]=train.content.apply(lambda x : str(x)[-1].isdigit())\n",
    "test[\"title_endswith_number\"]=test.title.apply(lambda x : str(x)[-1].isdigit())\n",
    "test[\"content_endswith_number\"]=test.content.apply(lambda x : str(x)[-1].isdigit())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title length \n",
    "train[\"title_length\"] = train['title'].apply(lambda x : len(x))\n",
    "test[\"title_length\"] = test['title'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content length \n",
    "train['content_length'] = train['content'].apply(lambda x: len(x))\n",
    "test['content_length'] = test['content'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average length of word in title \n",
    "train['title_mean_length'] = train['title_tokenized'].apply(lambda x: np.mean([len(w) for w in x])) \n",
    "test['title_mean_length'] = test['title_tokenized'].apply(lambda x: np.mean([len(w) for w in x])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 29), (142565, 29))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract TF-IDF 1-3 ngram features, then apply truncated SVD to incorporate into our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_tokenizer(text):  \n",
    "    tokens_mecab = mecab.morphs(text) \n",
    "    return tokens_mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for singular value decomposition  \n",
    "tfidf_vec = TfidfVectorizer(tokenizer = mecab_tokenizer, ngram_range = (1,3)) \n",
    "full_tfidf_title = tfidf_vec.fit_transform(train['title'].values.tolist() + test['title'].values.tolist())\n",
    "train_tfidf_title = tfidf_vec.transform(train['title'].values.tolist()) \n",
    "test_tfidf_title = tfidf_vec.transform(test['title'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((261310, 170537), (118745, 170537), (142565, 170537))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tfidf_title.shape, train_tfidf_title.shape, test_tfidf_title.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for singular value decomposition  \n",
    "tfidf_vec = TfidfVectorizer(tokenizer = mecab_tokenizer, ngram_range = (1,3)) \n",
    "full_tfidf_content = tfidf_vec.fit_transform(train['content'].values.tolist() + test['content'].values.tolist())\n",
    "train_tfidf_content = tfidf_vec.transform(train['content'].values.tolist()) \n",
    "test_tfidf_content = tfidf_vec.transform(test['content'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((261310, 2465227), (118745, 2465227), (142565, 2465227))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tfidf_content.shape, train_tfidf_content.shape, test_tfidf_content.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_obj = TruncatedSVD(n_components = 20, algorithm = 'arpack') \n",
    "svd_obj.fit(full_tfidf_title)\n",
    "train_svd_title = pd.DataFrame(svd_obj.transform(train_tfidf_title)) \n",
    "test_svd_title = pd.DataFrame(svd_obj.transform(test_tfidf_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svd_title.columns = ['svd_word_title_' + str(i) for i in range(20)] \n",
    "test_svd_title.columns = ['svd_word_title_' + str(i) for i in range(20)] \n",
    "\n",
    "train = pd.concat([train, train_svd_title], axis = 1) \n",
    "test = pd.concat([test, test_svd_title], axis = 1)  \n",
    "\n",
    "# del full_tfidf_title, train_tfidf_title, test_tfidf_title, train_svd_title, test_svd_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_obj = TruncatedSVD(n_components = 20, algorithm = 'arpack') \n",
    "svd_obj.fit(full_tfidf_content) \n",
    "train_svd_content = pd.DataFrame(svd_obj.transform(train_tfidf_content)) \n",
    "test_svd_content = pd.DataFrame(svd_obj.transform(test_tfidf_content)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svd_content.columns = ['svd_word_content_' + str(i) for i in range(20)] \n",
    "test_svd_content.columns = ['svd_word_content_' + str(i) for i in range(20)] \n",
    "\n",
    "train = pd.concat([train, train_svd_content], axis = 1) \n",
    "test = pd.concat([test, test_svd_content], axis = 1)  \n",
    "\n",
    "# del full_tfidf_content, train_tfidf_content, test_tfidf_content, train_svd_content, test_svd_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 69), (142565, 69))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the dataframe shape \n",
    "train.shape, test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unnecessary dataframes \n",
    "del train_svd_title, test_svd_title, train_svd_content, test_svd_content, full_tfidf_title, full_tfidf_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Count vectorizer 1-3 ngram, then fit MNB and SVC to add to our dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "cvec_full = CountVectorizer(tokenizer = mecab_tokenizer, ngram_range = (1,3)) \n",
    "cvec_full.fit(train['title'].values.tolist() + test['title'].values.tolist()) \n",
    "train_cvec_title = cvec_full.transform(train['title'].values.tolist()) \n",
    "test_cvec_title = cvec_full.transform(test['title'].values.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 170537), (142565, 170537))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cvec_title.shape, test_cvec_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_full_content = CountVectorizer(tokenizer = mecab_tokenizer, ngram_range = (1,3)) \n",
    "cvec_full_content.fit(train['content'].values.tolist() + test['content'].values.tolist()) \n",
    "train_cvec_content = cvec_full_content.transform(train['content'].values.tolist()) \n",
    "test_cvec_content = cvec_full_content.transform(test['content'].values.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 2465227), (142565, 2465227))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cvec_content.shape, test_cvec_content.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMNB(train_x, train_y, test_x, test_y, test_x2):  \n",
    "    model = MultinomialNB()\n",
    "    model.fit(train_x, train_y)\n",
    "    pred_test_y = model.predict_proba(test_x) \n",
    "    pred_test_y2 = model.predict_proba(test_x2) \n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118745,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train['info'] \n",
    "y_train = np.asarray(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB for tifidf title matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [] \n",
    "pred_full_test = 0 \n",
    "pred_train = np.zeros([train.shape[0],2]) \n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 990101) \n",
    "for train_idx, val_idx in kf.split(train, y_train):\n",
    "    train_x, val_x = train_tfidf_title[train_idx], train_tfidf_title[val_idx] \n",
    "    train_y, val_y = y_train[train_idx], y_train[val_idx] \n",
    "    pred_val_y, pred_test_y, model = runMNB(train_x, train_y, val_x, val_y, test_tfidf_title) \n",
    "    pred_full_test = pred_full_test + pred_test_y \n",
    "    pred_train[val_idx,:] = pred_val_y   \n",
    "pred_full_test /= 5.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nb_tfidf_real_title'] = pred_train[:,0] \n",
    "train['nb_tfidf_fake_title'] = pred_train[:,1] \n",
    "test['nb_tfidf_real_title'] = pred_full_test[:,0] \n",
    "test['nb_tfidf_fake_title'] = pred_full_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB for tfidif content matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [] \n",
    "pred_full_test = 0 \n",
    "pred_train = np.zeros([train.shape[0],2]) \n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 990101) \n",
    "for train_idx, val_idx in kf.split(train, y_train):\n",
    "    train_x, val_x = train_tfidf_content[train_idx], train_tfidf_content[val_idx] \n",
    "    train_y, val_y = y_train[train_idx], y_train[val_idx] \n",
    "    pred_val_y, pred_test_y, model = runMNB(train_x, train_y, val_x, val_y, test_tfidf_content) \n",
    "    pred_full_test = pred_full_test + pred_test_y \n",
    "    pred_train[val_idx,:] = pred_val_y   \n",
    "pred_full_test /= 5.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nb_tfidf_real_content'] = pred_train[:,0] \n",
    "train['nb_tfidf_fake_content'] = pred_train[:,1] \n",
    "test['nb_tfidf_real_content'] = pred_full_test[:,0]  \n",
    "test['nb_tfidf_fake_content'] = pred_full_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 73), (142565, 73))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB for cvec title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [] \n",
    "pred_full_test = 0 \n",
    "pred_train = np.zeros([train.shape[0],2]) \n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 990101) \n",
    "for train_idx, val_idx in kf.split(train, y_train):\n",
    "    train_x, val_x = train_cvec_title[train_idx], train_cvec_title[val_idx] \n",
    "    train_y, val_y = y_train[train_idx], y_train[val_idx] \n",
    "    pred_val_y, pred_test_y, model = runMNB(train_x, train_y, val_x, val_y, test_cvec_title) \n",
    "    pred_full_test = pred_full_test + pred_test_y \n",
    "    pred_train[val_idx,:] = pred_val_y   \n",
    "pred_full_test /= 5.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nb_cvec_real_title'] = pred_train[:,0] \n",
    "train['nb_cvec_fake_title'] = pred_train[:,1] \n",
    "test['nb_cvec_real_title'] = pred_full_test[:,0]  \n",
    "test['nb_cvec_fake_title'] = pred_full_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNB for cvec content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [] \n",
    "pred_full_test = 0 \n",
    "pred_train = np.zeros([train.shape[0],2]) \n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 990101) \n",
    "for train_idx, val_idx in kf.split(train, y_train):\n",
    "    train_x, val_x = train_cvec_content[train_idx], train_cvec_content[val_idx] \n",
    "    train_y, val_y = y_train[train_idx], y_train[val_idx] \n",
    "    pred_val_y, pred_test_y, model = runMNB(train_x, train_y, val_x, val_y, test_cvec_content) \n",
    "    pred_full_test = pred_full_test + pred_test_y \n",
    "    pred_train[val_idx,:] = pred_val_y   \n",
    "pred_full_test /= 5.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nb_cvec_real_content'] = pred_train[:,0] \n",
    "train['nb_cvec_fake_content'] = pred_train[:,1] \n",
    "test['nb_cvec_real_content'] = pred_full_test[:,0]  \n",
    "test['nb_cvec_fake_content'] = pred_full_test[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of the dataframe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((118745, 77), (142565, 77))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Final shape of the dataframe\") \n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LightGBM Model \n",
    "\n",
    "We will begin training at this step. We use the light gbm model for now as it is fast and also pretty powerful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['n_id','info','title','content','title_tokenized','content_tokenized'], axis = 1) \n",
    "y_train = train['info'] \n",
    "\n",
    "x_test = test.drop(['n_id','id','title','content','title_tokenized','content_tokenized'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 71), (118745,), (142565, 71))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.asarray(x_train) \n",
    "y_train = np.asarray(y_train) \n",
    "x_test = np.asarray(x_test) \n",
    "\n",
    "x_train.shape, y_train.shape, x_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe \n",
    "np.save('x_train_latest.npy',x_train) \n",
    "np.save('y_train_latest.npy',y_train) \n",
    "np.save('x_test_latest.npy',x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 37546, number of negative: 57450\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13493\n",
      "[LightGBM] [Info] Number of data points in the train set: 94996, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.395238 -> initscore=-0.425348\n",
      "[LightGBM] [Info] Start training from score -0.425348\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.583261\n",
      "[20]\tvalid_0's binary_logloss: 0.511219\n",
      "[30]\tvalid_0's binary_logloss: 0.450686\n",
      "[40]\tvalid_0's binary_logloss: 0.399565\n",
      "[50]\tvalid_0's binary_logloss: 0.355451\n",
      "[60]\tvalid_0's binary_logloss: 0.317338\n",
      "[70]\tvalid_0's binary_logloss: 0.284186\n",
      "[80]\tvalid_0's binary_logloss: 0.255177\n",
      "[90]\tvalid_0's binary_logloss: 0.229712\n",
      "[100]\tvalid_0's binary_logloss: 0.207269\n",
      "[110]\tvalid_0's binary_logloss: 0.187406\n",
      "[120]\tvalid_0's binary_logloss: 0.169756\n",
      "[130]\tvalid_0's binary_logloss: 0.154111\n",
      "[140]\tvalid_0's binary_logloss: 0.140131\n",
      "[150]\tvalid_0's binary_logloss: 0.127617\n",
      "[160]\tvalid_0's binary_logloss: 0.116434\n",
      "[170]\tvalid_0's binary_logloss: 0.106432\n",
      "[180]\tvalid_0's binary_logloss: 0.0974442\n",
      "[190]\tvalid_0's binary_logloss: 0.0893908\n",
      "[200]\tvalid_0's binary_logloss: 0.0821769\n",
      "[210]\tvalid_0's binary_logloss: 0.0756168\n",
      "[220]\tvalid_0's binary_logloss: 0.0696896\n",
      "[230]\tvalid_0's binary_logloss: 0.0643144\n",
      "[240]\tvalid_0's binary_logloss: 0.0594605\n",
      "[250]\tvalid_0's binary_logloss: 0.0550816\n",
      "[260]\tvalid_0's binary_logloss: 0.0511389\n",
      "[270]\tvalid_0's binary_logloss: 0.0475609\n",
      "[280]\tvalid_0's binary_logloss: 0.0442612\n",
      "[290]\tvalid_0's binary_logloss: 0.041327\n",
      "[300]\tvalid_0's binary_logloss: 0.0386278\n",
      "[310]\tvalid_0's binary_logloss: 0.0361775\n",
      "[320]\tvalid_0's binary_logloss: 0.0339648\n",
      "[330]\tvalid_0's binary_logloss: 0.0319716\n",
      "[340]\tvalid_0's binary_logloss: 0.0301273\n",
      "[350]\tvalid_0's binary_logloss: 0.0284629\n",
      "[360]\tvalid_0's binary_logloss: 0.026942\n",
      "[370]\tvalid_0's binary_logloss: 0.0255686\n",
      "[380]\tvalid_0's binary_logloss: 0.0243617\n",
      "[390]\tvalid_0's binary_logloss: 0.0232502\n",
      "[400]\tvalid_0's binary_logloss: 0.0222446\n",
      "[410]\tvalid_0's binary_logloss: 0.0213189\n",
      "[420]\tvalid_0's binary_logloss: 0.0205054\n",
      "[430]\tvalid_0's binary_logloss: 0.0197162\n",
      "[440]\tvalid_0's binary_logloss: 0.0189981\n",
      "[450]\tvalid_0's binary_logloss: 0.0183199\n",
      "[460]\tvalid_0's binary_logloss: 0.0177351\n",
      "[470]\tvalid_0's binary_logloss: 0.017165\n",
      "[480]\tvalid_0's binary_logloss: 0.0166422\n",
      "[490]\tvalid_0's binary_logloss: 0.0161872\n",
      "[500]\tvalid_0's binary_logloss: 0.0157302\n",
      "[510]\tvalid_0's binary_logloss: 0.0153604\n",
      "[520]\tvalid_0's binary_logloss: 0.015013\n",
      "[530]\tvalid_0's binary_logloss: 0.01466\n",
      "[540]\tvalid_0's binary_logloss: 0.0143567\n",
      "[550]\tvalid_0's binary_logloss: 0.0140702\n",
      "[560]\tvalid_0's binary_logloss: 0.0138417\n",
      "[570]\tvalid_0's binary_logloss: 0.0136143\n",
      "[580]\tvalid_0's binary_logloss: 0.0134046\n",
      "[590]\tvalid_0's binary_logloss: 0.013238\n",
      "[600]\tvalid_0's binary_logloss: 0.0130371\n",
      "[610]\tvalid_0's binary_logloss: 0.0128715\n",
      "[620]\tvalid_0's binary_logloss: 0.0126725\n",
      "[630]\tvalid_0's binary_logloss: 0.0124956\n",
      "[640]\tvalid_0's binary_logloss: 0.0123219\n",
      "[650]\tvalid_0's binary_logloss: 0.0122196\n",
      "[660]\tvalid_0's binary_logloss: 0.012106\n",
      "[670]\tvalid_0's binary_logloss: 0.0120069\n",
      "[680]\tvalid_0's binary_logloss: 0.011928\n",
      "[690]\tvalid_0's binary_logloss: 0.0118267\n",
      "[700]\tvalid_0's binary_logloss: 0.0117551\n",
      "[710]\tvalid_0's binary_logloss: 0.0116848\n",
      "[720]\tvalid_0's binary_logloss: 0.0116142\n",
      "[730]\tvalid_0's binary_logloss: 0.0115555\n",
      "[740]\tvalid_0's binary_logloss: 0.0115098\n",
      "[750]\tvalid_0's binary_logloss: 0.0114344\n",
      "[760]\tvalid_0's binary_logloss: 0.0113855\n",
      "[770]\tvalid_0's binary_logloss: 0.0113527\n",
      "[780]\tvalid_0's binary_logloss: 0.0112863\n",
      "[790]\tvalid_0's binary_logloss: 0.0112656\n",
      "[800]\tvalid_0's binary_logloss: 0.0112473\n",
      "[810]\tvalid_0's binary_logloss: 0.0112083\n",
      "[820]\tvalid_0's binary_logloss: 0.0111773\n",
      "[830]\tvalid_0's binary_logloss: 0.011123\n",
      "[840]\tvalid_0's binary_logloss: 0.011116\n",
      "[850]\tvalid_0's binary_logloss: 0.0111139\n",
      "[860]\tvalid_0's binary_logloss: 0.0110792\n",
      "[870]\tvalid_0's binary_logloss: 0.0110396\n",
      "[880]\tvalid_0's binary_logloss: 0.0110139\n",
      "[890]\tvalid_0's binary_logloss: 0.0109948\n",
      "[900]\tvalid_0's binary_logloss: 0.010981\n",
      "[910]\tvalid_0's binary_logloss: 0.0109685\n",
      "[920]\tvalid_0's binary_logloss: 0.0109239\n",
      "[930]\tvalid_0's binary_logloss: 0.0108981\n",
      "[940]\tvalid_0's binary_logloss: 0.0109043\n",
      "[950]\tvalid_0's binary_logloss: 0.0109086\n",
      "[960]\tvalid_0's binary_logloss: 0.010882\n",
      "[970]\tvalid_0's binary_logloss: 0.010861\n",
      "[980]\tvalid_0's binary_logloss: 0.0108566\n",
      "[990]\tvalid_0's binary_logloss: 0.0108603\n",
      "[1000]\tvalid_0's binary_logloss: 0.0108329\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.0108329\n",
      "[LightGBM] [Info] Number of positive: 37546, number of negative: 57450\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13495\n",
      "[LightGBM] [Info] Number of data points in the train set: 94996, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.395238 -> initscore=-0.425348\n",
      "[LightGBM] [Info] Start training from score -0.425348\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.583067\n",
      "[20]\tvalid_0's binary_logloss: 0.51079\n",
      "[30]\tvalid_0's binary_logloss: 0.450274\n",
      "[40]\tvalid_0's binary_logloss: 0.398944\n",
      "[50]\tvalid_0's binary_logloss: 0.354839\n",
      "[60]\tvalid_0's binary_logloss: 0.316781\n",
      "[70]\tvalid_0's binary_logloss: 0.283604\n",
      "[80]\tvalid_0's binary_logloss: 0.25456\n",
      "[90]\tvalid_0's binary_logloss: 0.229086\n",
      "[100]\tvalid_0's binary_logloss: 0.206611\n",
      "[110]\tvalid_0's binary_logloss: 0.186749\n",
      "[120]\tvalid_0's binary_logloss: 0.169077\n",
      "[130]\tvalid_0's binary_logloss: 0.153463\n",
      "[140]\tvalid_0's binary_logloss: 0.139494\n",
      "[150]\tvalid_0's binary_logloss: 0.127008\n",
      "[160]\tvalid_0's binary_logloss: 0.115821\n",
      "[170]\tvalid_0's binary_logloss: 0.105845\n",
      "[180]\tvalid_0's binary_logloss: 0.0968461\n",
      "[190]\tvalid_0's binary_logloss: 0.088745\n",
      "[200]\tvalid_0's binary_logloss: 0.0815157\n",
      "[210]\tvalid_0's binary_logloss: 0.0749555\n",
      "[220]\tvalid_0's binary_logloss: 0.0690729\n",
      "[230]\tvalid_0's binary_logloss: 0.0637288\n",
      "[240]\tvalid_0's binary_logloss: 0.0589031\n",
      "[250]\tvalid_0's binary_logloss: 0.0545239\n",
      "[260]\tvalid_0's binary_logloss: 0.0505851\n",
      "[270]\tvalid_0's binary_logloss: 0.0470057\n",
      "[280]\tvalid_0's binary_logloss: 0.0437563\n",
      "[290]\tvalid_0's binary_logloss: 0.0408269\n",
      "[300]\tvalid_0's binary_logloss: 0.0381496\n",
      "[310]\tvalid_0's binary_logloss: 0.0357663\n",
      "[320]\tvalid_0's binary_logloss: 0.0335452\n",
      "[330]\tvalid_0's binary_logloss: 0.0315698\n",
      "[340]\tvalid_0's binary_logloss: 0.0297952\n",
      "[350]\tvalid_0's binary_logloss: 0.0281464\n",
      "[360]\tvalid_0's binary_logloss: 0.0266418\n",
      "[370]\tvalid_0's binary_logloss: 0.0253018\n",
      "[380]\tvalid_0's binary_logloss: 0.0240751\n",
      "[390]\tvalid_0's binary_logloss: 0.0229165\n",
      "[400]\tvalid_0's binary_logloss: 0.0219149\n",
      "[410]\tvalid_0's binary_logloss: 0.0209919\n",
      "[420]\tvalid_0's binary_logloss: 0.0201222\n",
      "[430]\tvalid_0's binary_logloss: 0.0192882\n",
      "[440]\tvalid_0's binary_logloss: 0.0185543\n",
      "[450]\tvalid_0's binary_logloss: 0.017895\n",
      "[460]\tvalid_0's binary_logloss: 0.0172847\n",
      "[470]\tvalid_0's binary_logloss: 0.0166792\n",
      "[480]\tvalid_0's binary_logloss: 0.0161475\n",
      "[490]\tvalid_0's binary_logloss: 0.015681\n",
      "[500]\tvalid_0's binary_logloss: 0.0152397\n",
      "[510]\tvalid_0's binary_logloss: 0.0148404\n",
      "[520]\tvalid_0's binary_logloss: 0.0144521\n",
      "[530]\tvalid_0's binary_logloss: 0.0141075\n",
      "[540]\tvalid_0's binary_logloss: 0.0138064\n",
      "[550]\tvalid_0's binary_logloss: 0.0135114\n",
      "[560]\tvalid_0's binary_logloss: 0.0132559\n",
      "[570]\tvalid_0's binary_logloss: 0.0129994\n",
      "[580]\tvalid_0's binary_logloss: 0.0127648\n",
      "[590]\tvalid_0's binary_logloss: 0.0125582\n",
      "[600]\tvalid_0's binary_logloss: 0.012364\n",
      "[610]\tvalid_0's binary_logloss: 0.0121937\n",
      "[620]\tvalid_0's binary_logloss: 0.0120217\n",
      "[630]\tvalid_0's binary_logloss: 0.0118628\n",
      "[640]\tvalid_0's binary_logloss: 0.0117134\n",
      "[650]\tvalid_0's binary_logloss: 0.0115721\n",
      "[660]\tvalid_0's binary_logloss: 0.0114389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[670]\tvalid_0's binary_logloss: 0.0113049\n",
      "[680]\tvalid_0's binary_logloss: 0.01119\n",
      "[690]\tvalid_0's binary_logloss: 0.0111126\n",
      "[700]\tvalid_0's binary_logloss: 0.0110243\n",
      "[710]\tvalid_0's binary_logloss: 0.0109368\n",
      "[720]\tvalid_0's binary_logloss: 0.0108497\n",
      "[730]\tvalid_0's binary_logloss: 0.0107712\n",
      "[740]\tvalid_0's binary_logloss: 0.0107122\n",
      "[750]\tvalid_0's binary_logloss: 0.0106566\n",
      "[760]\tvalid_0's binary_logloss: 0.0105969\n",
      "[770]\tvalid_0's binary_logloss: 0.0105451\n",
      "[780]\tvalid_0's binary_logloss: 0.0104796\n",
      "[790]\tvalid_0's binary_logloss: 0.0104215\n",
      "[800]\tvalid_0's binary_logloss: 0.0103993\n",
      "[810]\tvalid_0's binary_logloss: 0.0103787\n",
      "[820]\tvalid_0's binary_logloss: 0.0103234\n",
      "[830]\tvalid_0's binary_logloss: 0.0102729\n",
      "[840]\tvalid_0's binary_logloss: 0.0102306\n",
      "[850]\tvalid_0's binary_logloss: 0.010212\n",
      "[860]\tvalid_0's binary_logloss: 0.0101782\n",
      "[870]\tvalid_0's binary_logloss: 0.0101578\n",
      "[880]\tvalid_0's binary_logloss: 0.010137\n",
      "[890]\tvalid_0's binary_logloss: 0.0100807\n",
      "[900]\tvalid_0's binary_logloss: 0.0100887\n",
      "[910]\tvalid_0's binary_logloss: 0.0100952\n",
      "[920]\tvalid_0's binary_logloss: 0.0100801\n",
      "[930]\tvalid_0's binary_logloss: 0.0100582\n",
      "[940]\tvalid_0's binary_logloss: 0.0100273\n",
      "[950]\tvalid_0's binary_logloss: 0.0100045\n",
      "[960]\tvalid_0's binary_logloss: 0.00996709\n",
      "[970]\tvalid_0's binary_logloss: 0.00995121\n",
      "[980]\tvalid_0's binary_logloss: 0.00992781\n",
      "[990]\tvalid_0's binary_logloss: 0.00992762\n",
      "[1000]\tvalid_0's binary_logloss: 0.00987977\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.00987977\n",
      "[LightGBM] [Info] Number of positive: 37546, number of negative: 57450\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13493\n",
      "[LightGBM] [Info] Number of data points in the train set: 94996, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.395238 -> initscore=-0.425348\n",
      "[LightGBM] [Info] Start training from score -0.425348\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.583466\n",
      "[20]\tvalid_0's binary_logloss: 0.511532\n",
      "[30]\tvalid_0's binary_logloss: 0.451289\n",
      "[40]\tvalid_0's binary_logloss: 0.400131\n",
      "[50]\tvalid_0's binary_logloss: 0.35632\n",
      "[60]\tvalid_0's binary_logloss: 0.318338\n",
      "[70]\tvalid_0's binary_logloss: 0.285445\n",
      "[80]\tvalid_0's binary_logloss: 0.256613\n",
      "[90]\tvalid_0's binary_logloss: 0.231309\n",
      "[100]\tvalid_0's binary_logloss: 0.208909\n",
      "[110]\tvalid_0's binary_logloss: 0.189087\n",
      "[120]\tvalid_0's binary_logloss: 0.171555\n",
      "[130]\tvalid_0's binary_logloss: 0.155984\n",
      "[140]\tvalid_0's binary_logloss: 0.142094\n",
      "[150]\tvalid_0's binary_logloss: 0.129634\n",
      "[160]\tvalid_0's binary_logloss: 0.118508\n",
      "[170]\tvalid_0's binary_logloss: 0.108458\n",
      "[180]\tvalid_0's binary_logloss: 0.099462\n",
      "[190]\tvalid_0's binary_logloss: 0.0913857\n",
      "[200]\tvalid_0's binary_logloss: 0.0841032\n",
      "[210]\tvalid_0's binary_logloss: 0.0775789\n",
      "[220]\tvalid_0's binary_logloss: 0.0716878\n",
      "[230]\tvalid_0's binary_logloss: 0.0663888\n",
      "[240]\tvalid_0's binary_logloss: 0.0615979\n",
      "[250]\tvalid_0's binary_logloss: 0.0572422\n",
      "[260]\tvalid_0's binary_logloss: 0.0533127\n",
      "[270]\tvalid_0's binary_logloss: 0.0497694\n",
      "[280]\tvalid_0's binary_logloss: 0.0465061\n",
      "[290]\tvalid_0's binary_logloss: 0.0435612\n",
      "[300]\tvalid_0's binary_logloss: 0.040859\n",
      "[310]\tvalid_0's binary_logloss: 0.0384025\n",
      "[320]\tvalid_0's binary_logloss: 0.0362252\n",
      "[330]\tvalid_0's binary_logloss: 0.0341824\n",
      "[340]\tvalid_0's binary_logloss: 0.0323926\n",
      "[350]\tvalid_0's binary_logloss: 0.0307024\n",
      "[360]\tvalid_0's binary_logloss: 0.0291468\n",
      "[370]\tvalid_0's binary_logloss: 0.0278021\n",
      "[380]\tvalid_0's binary_logloss: 0.0265443\n",
      "[390]\tvalid_0's binary_logloss: 0.0254078\n",
      "[400]\tvalid_0's binary_logloss: 0.0243224\n",
      "[410]\tvalid_0's binary_logloss: 0.023337\n",
      "[420]\tvalid_0's binary_logloss: 0.0224644\n",
      "[430]\tvalid_0's binary_logloss: 0.0216663\n",
      "[440]\tvalid_0's binary_logloss: 0.0209077\n",
      "[450]\tvalid_0's binary_logloss: 0.0202175\n",
      "[460]\tvalid_0's binary_logloss: 0.0195765\n",
      "[470]\tvalid_0's binary_logloss: 0.0189791\n",
      "[480]\tvalid_0's binary_logloss: 0.0184403\n",
      "[490]\tvalid_0's binary_logloss: 0.0179573\n",
      "[500]\tvalid_0's binary_logloss: 0.017516\n",
      "[510]\tvalid_0's binary_logloss: 0.0170807\n",
      "[520]\tvalid_0's binary_logloss: 0.0166791\n",
      "[530]\tvalid_0's binary_logloss: 0.0163504\n",
      "[540]\tvalid_0's binary_logloss: 0.0160359\n",
      "[550]\tvalid_0's binary_logloss: 0.0157389\n",
      "[560]\tvalid_0's binary_logloss: 0.0154468\n",
      "[570]\tvalid_0's binary_logloss: 0.0151823\n",
      "[580]\tvalid_0's binary_logloss: 0.0149573\n",
      "[590]\tvalid_0's binary_logloss: 0.0147265\n",
      "[600]\tvalid_0's binary_logloss: 0.0145312\n",
      "[610]\tvalid_0's binary_logloss: 0.014331\n",
      "[620]\tvalid_0's binary_logloss: 0.0141586\n",
      "[630]\tvalid_0's binary_logloss: 0.0140105\n",
      "[640]\tvalid_0's binary_logloss: 0.0138767\n",
      "[650]\tvalid_0's binary_logloss: 0.0137574\n",
      "[660]\tvalid_0's binary_logloss: 0.0136268\n",
      "[670]\tvalid_0's binary_logloss: 0.013558\n",
      "[680]\tvalid_0's binary_logloss: 0.0134607\n",
      "[690]\tvalid_0's binary_logloss: 0.0133544\n",
      "[700]\tvalid_0's binary_logloss: 0.0132709\n",
      "[710]\tvalid_0's binary_logloss: 0.0131536\n",
      "[720]\tvalid_0's binary_logloss: 0.0130486\n",
      "[730]\tvalid_0's binary_logloss: 0.0130246\n",
      "[740]\tvalid_0's binary_logloss: 0.0129478\n",
      "[750]\tvalid_0's binary_logloss: 0.0129027\n",
      "[760]\tvalid_0's binary_logloss: 0.0128122\n",
      "[770]\tvalid_0's binary_logloss: 0.0127475\n",
      "[780]\tvalid_0's binary_logloss: 0.0127146\n",
      "[790]\tvalid_0's binary_logloss: 0.0126353\n",
      "[800]\tvalid_0's binary_logloss: 0.0125681\n",
      "[810]\tvalid_0's binary_logloss: 0.0125233\n",
      "[820]\tvalid_0's binary_logloss: 0.0124981\n",
      "[830]\tvalid_0's binary_logloss: 0.0124482\n",
      "[840]\tvalid_0's binary_logloss: 0.0124357\n",
      "[850]\tvalid_0's binary_logloss: 0.0123915\n",
      "[860]\tvalid_0's binary_logloss: 0.0123409\n",
      "[870]\tvalid_0's binary_logloss: 0.0123058\n",
      "[880]\tvalid_0's binary_logloss: 0.0122653\n",
      "[890]\tvalid_0's binary_logloss: 0.0122223\n",
      "[900]\tvalid_0's binary_logloss: 0.0122163\n",
      "[910]\tvalid_0's binary_logloss: 0.0122158\n",
      "[920]\tvalid_0's binary_logloss: 0.0121904\n",
      "[930]\tvalid_0's binary_logloss: 0.0121722\n",
      "[940]\tvalid_0's binary_logloss: 0.0121321\n",
      "[950]\tvalid_0's binary_logloss: 0.0121261\n",
      "[960]\tvalid_0's binary_logloss: 0.0121047\n",
      "[970]\tvalid_0's binary_logloss: 0.0120872\n",
      "[980]\tvalid_0's binary_logloss: 0.0120433\n",
      "[990]\tvalid_0's binary_logloss: 0.0120311\n",
      "[1000]\tvalid_0's binary_logloss: 0.0120106\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's binary_logloss: 0.0120044\n",
      "[LightGBM] [Info] Number of positive: 37545, number of negative: 57451\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13494\n",
      "[LightGBM] [Info] Number of data points in the train set: 94996, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.395227 -> initscore=-0.425392\n",
      "[LightGBM] [Info] Start training from score -0.425392\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.583626\n",
      "[20]\tvalid_0's binary_logloss: 0.511788\n",
      "[30]\tvalid_0's binary_logloss: 0.451622\n",
      "[40]\tvalid_0's binary_logloss: 0.400434\n",
      "[50]\tvalid_0's binary_logloss: 0.356546\n",
      "[60]\tvalid_0's binary_logloss: 0.318594\n",
      "[70]\tvalid_0's binary_logloss: 0.28566\n",
      "[80]\tvalid_0's binary_logloss: 0.256923\n",
      "[90]\tvalid_0's binary_logloss: 0.231605\n",
      "[100]\tvalid_0's binary_logloss: 0.20927\n",
      "[110]\tvalid_0's binary_logloss: 0.189479\n",
      "[120]\tvalid_0's binary_logloss: 0.171937\n",
      "[130]\tvalid_0's binary_logloss: 0.156348\n",
      "[140]\tvalid_0's binary_logloss: 0.142439\n",
      "[150]\tvalid_0's binary_logloss: 0.130008\n",
      "[160]\tvalid_0's binary_logloss: 0.118905\n",
      "[170]\tvalid_0's binary_logloss: 0.108934\n",
      "[180]\tvalid_0's binary_logloss: 0.0999991\n",
      "[190]\tvalid_0's binary_logloss: 0.0919836\n",
      "[200]\tvalid_0's binary_logloss: 0.0847182\n",
      "[210]\tvalid_0's binary_logloss: 0.0782368\n",
      "[220]\tvalid_0's binary_logloss: 0.0723183\n",
      "[230]\tvalid_0's binary_logloss: 0.0670127\n",
      "[240]\tvalid_0's binary_logloss: 0.0621887\n",
      "[250]\tvalid_0's binary_logloss: 0.0578263\n",
      "[260]\tvalid_0's binary_logloss: 0.0539148\n",
      "[270]\tvalid_0's binary_logloss: 0.0503131\n",
      "[280]\tvalid_0's binary_logloss: 0.0471597\n",
      "[290]\tvalid_0's binary_logloss: 0.0442391\n",
      "[300]\tvalid_0's binary_logloss: 0.0416101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310]\tvalid_0's binary_logloss: 0.0391982\n",
      "[320]\tvalid_0's binary_logloss: 0.0370112\n",
      "[330]\tvalid_0's binary_logloss: 0.0350093\n",
      "[340]\tvalid_0's binary_logloss: 0.0331881\n",
      "[350]\tvalid_0's binary_logloss: 0.0315374\n",
      "[360]\tvalid_0's binary_logloss: 0.0300673\n",
      "[370]\tvalid_0's binary_logloss: 0.0287043\n",
      "[380]\tvalid_0's binary_logloss: 0.0274822\n",
      "[390]\tvalid_0's binary_logloss: 0.0263705\n",
      "[400]\tvalid_0's binary_logloss: 0.0253763\n",
      "[410]\tvalid_0's binary_logloss: 0.0244338\n",
      "[420]\tvalid_0's binary_logloss: 0.0234847\n",
      "[430]\tvalid_0's binary_logloss: 0.022652\n",
      "[440]\tvalid_0's binary_logloss: 0.0219171\n",
      "[450]\tvalid_0's binary_logloss: 0.0212138\n",
      "[460]\tvalid_0's binary_logloss: 0.0206162\n",
      "[470]\tvalid_0's binary_logloss: 0.0200923\n",
      "[480]\tvalid_0's binary_logloss: 0.0195468\n",
      "[490]\tvalid_0's binary_logloss: 0.0190835\n",
      "[500]\tvalid_0's binary_logloss: 0.0186559\n",
      "[510]\tvalid_0's binary_logloss: 0.0182476\n",
      "[520]\tvalid_0's binary_logloss: 0.0178784\n",
      "[530]\tvalid_0's binary_logloss: 0.0175164\n",
      "[540]\tvalid_0's binary_logloss: 0.0172551\n",
      "[550]\tvalid_0's binary_logloss: 0.0169873\n",
      "[560]\tvalid_0's binary_logloss: 0.0167107\n",
      "[570]\tvalid_0's binary_logloss: 0.0164438\n",
      "[580]\tvalid_0's binary_logloss: 0.0161667\n",
      "[590]\tvalid_0's binary_logloss: 0.0159342\n",
      "[600]\tvalid_0's binary_logloss: 0.0158091\n",
      "[610]\tvalid_0's binary_logloss: 0.0156015\n",
      "[620]\tvalid_0's binary_logloss: 0.0154944\n",
      "[630]\tvalid_0's binary_logloss: 0.0153637\n",
      "[640]\tvalid_0's binary_logloss: 0.0152015\n",
      "[650]\tvalid_0's binary_logloss: 0.0150632\n",
      "[660]\tvalid_0's binary_logloss: 0.0149345\n",
      "[670]\tvalid_0's binary_logloss: 0.0148054\n",
      "[680]\tvalid_0's binary_logloss: 0.0147296\n",
      "[690]\tvalid_0's binary_logloss: 0.0146146\n",
      "[700]\tvalid_0's binary_logloss: 0.0145169\n",
      "[710]\tvalid_0's binary_logloss: 0.0144091\n",
      "[720]\tvalid_0's binary_logloss: 0.0143329\n",
      "[730]\tvalid_0's binary_logloss: 0.0142486\n",
      "[740]\tvalid_0's binary_logloss: 0.0142192\n",
      "[750]\tvalid_0's binary_logloss: 0.0141342\n",
      "[760]\tvalid_0's binary_logloss: 0.0140563\n",
      "[770]\tvalid_0's binary_logloss: 0.0140172\n",
      "[780]\tvalid_0's binary_logloss: 0.013976\n",
      "[790]\tvalid_0's binary_logloss: 0.0139041\n",
      "[800]\tvalid_0's binary_logloss: 0.0138453\n",
      "[810]\tvalid_0's binary_logloss: 0.0137911\n",
      "[820]\tvalid_0's binary_logloss: 0.0137551\n",
      "[830]\tvalid_0's binary_logloss: 0.0137476\n",
      "[840]\tvalid_0's binary_logloss: 0.0137444\n",
      "[850]\tvalid_0's binary_logloss: 0.0137366\n",
      "[860]\tvalid_0's binary_logloss: 0.0137351\n",
      "[870]\tvalid_0's binary_logloss: 0.0137075\n",
      "[880]\tvalid_0's binary_logloss: 0.0136741\n",
      "[890]\tvalid_0's binary_logloss: 0.0136528\n",
      "[900]\tvalid_0's binary_logloss: 0.0136263\n",
      "[910]\tvalid_0's binary_logloss: 0.013604\n",
      "[920]\tvalid_0's binary_logloss: 0.0135724\n",
      "[930]\tvalid_0's binary_logloss: 0.0135381\n",
      "[940]\tvalid_0's binary_logloss: 0.0135565\n",
      "[950]\tvalid_0's binary_logloss: 0.0135535\n",
      "[960]\tvalid_0's binary_logloss: 0.0135268\n",
      "[970]\tvalid_0's binary_logloss: 0.0135268\n",
      "[980]\tvalid_0's binary_logloss: 0.0135061\n",
      "[990]\tvalid_0's binary_logloss: 0.0135004\n",
      "[1000]\tvalid_0's binary_logloss: 0.013501\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[989]\tvalid_0's binary_logloss: 0.0134999\n",
      "[LightGBM] [Info] Number of positive: 37545, number of negative: 57451\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13497\n",
      "[LightGBM] [Info] Number of data points in the train set: 94996, number of used features: 71\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.395227 -> initscore=-0.425392\n",
      "[LightGBM] [Info] Start training from score -0.425392\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.583551\n",
      "[20]\tvalid_0's binary_logloss: 0.511581\n",
      "[30]\tvalid_0's binary_logloss: 0.451441\n",
      "[40]\tvalid_0's binary_logloss: 0.400285\n",
      "[50]\tvalid_0's binary_logloss: 0.356417\n",
      "[60]\tvalid_0's binary_logloss: 0.318443\n",
      "[70]\tvalid_0's binary_logloss: 0.28549\n",
      "[80]\tvalid_0's binary_logloss: 0.256603\n",
      "[90]\tvalid_0's binary_logloss: 0.231222\n",
      "[100]\tvalid_0's binary_logloss: 0.208777\n",
      "[110]\tvalid_0's binary_logloss: 0.188935\n",
      "[120]\tvalid_0's binary_logloss: 0.171399\n",
      "[130]\tvalid_0's binary_logloss: 0.155882\n",
      "[140]\tvalid_0's binary_logloss: 0.141932\n",
      "[150]\tvalid_0's binary_logloss: 0.129476\n",
      "[160]\tvalid_0's binary_logloss: 0.118269\n",
      "[170]\tvalid_0's binary_logloss: 0.108284\n",
      "[180]\tvalid_0's binary_logloss: 0.0992648\n",
      "[190]\tvalid_0's binary_logloss: 0.0911909\n",
      "[200]\tvalid_0's binary_logloss: 0.0839135\n",
      "[210]\tvalid_0's binary_logloss: 0.0773473\n",
      "[220]\tvalid_0's binary_logloss: 0.0713792\n",
      "[230]\tvalid_0's binary_logloss: 0.0660377\n",
      "[240]\tvalid_0's binary_logloss: 0.0612353\n",
      "[250]\tvalid_0's binary_logloss: 0.0568702\n",
      "[260]\tvalid_0's binary_logloss: 0.0529362\n",
      "[270]\tvalid_0's binary_logloss: 0.0493442\n",
      "[280]\tvalid_0's binary_logloss: 0.0460904\n",
      "[290]\tvalid_0's binary_logloss: 0.043164\n",
      "[300]\tvalid_0's binary_logloss: 0.0404802\n",
      "[310]\tvalid_0's binary_logloss: 0.0380329\n",
      "[320]\tvalid_0's binary_logloss: 0.0358714\n",
      "[330]\tvalid_0's binary_logloss: 0.0338819\n",
      "[340]\tvalid_0's binary_logloss: 0.0320535\n",
      "[350]\tvalid_0's binary_logloss: 0.0304176\n",
      "[360]\tvalid_0's binary_logloss: 0.0289079\n",
      "[370]\tvalid_0's binary_logloss: 0.0275228\n",
      "[380]\tvalid_0's binary_logloss: 0.0262643\n",
      "[390]\tvalid_0's binary_logloss: 0.0251421\n",
      "[400]\tvalid_0's binary_logloss: 0.0241266\n",
      "[410]\tvalid_0's binary_logloss: 0.0231964\n",
      "[420]\tvalid_0's binary_logloss: 0.0223516\n",
      "[430]\tvalid_0's binary_logloss: 0.0215693\n",
      "[440]\tvalid_0's binary_logloss: 0.0208568\n",
      "[450]\tvalid_0's binary_logloss: 0.0201849\n",
      "[460]\tvalid_0's binary_logloss: 0.019607\n",
      "[470]\tvalid_0's binary_logloss: 0.019069\n",
      "[480]\tvalid_0's binary_logloss: 0.01859\n",
      "[490]\tvalid_0's binary_logloss: 0.0181471\n",
      "[500]\tvalid_0's binary_logloss: 0.0177192\n",
      "[510]\tvalid_0's binary_logloss: 0.0173188\n",
      "[520]\tvalid_0's binary_logloss: 0.0169578\n",
      "[530]\tvalid_0's binary_logloss: 0.0166011\n",
      "[540]\tvalid_0's binary_logloss: 0.0163294\n",
      "[550]\tvalid_0's binary_logloss: 0.0160503\n",
      "[560]\tvalid_0's binary_logloss: 0.015789\n",
      "[570]\tvalid_0's binary_logloss: 0.0155515\n",
      "[580]\tvalid_0's binary_logloss: 0.0153366\n",
      "[590]\tvalid_0's binary_logloss: 0.0151373\n",
      "[600]\tvalid_0's binary_logloss: 0.0149604\n",
      "[610]\tvalid_0's binary_logloss: 0.0148026\n",
      "[620]\tvalid_0's binary_logloss: 0.0146386\n",
      "[630]\tvalid_0's binary_logloss: 0.0145024\n",
      "[640]\tvalid_0's binary_logloss: 0.0144063\n",
      "[650]\tvalid_0's binary_logloss: 0.0143153\n",
      "[660]\tvalid_0's binary_logloss: 0.0141836\n",
      "[670]\tvalid_0's binary_logloss: 0.014129\n",
      "[680]\tvalid_0's binary_logloss: 0.0140481\n",
      "[690]\tvalid_0's binary_logloss: 0.0139635\n",
      "[700]\tvalid_0's binary_logloss: 0.0139033\n",
      "[710]\tvalid_0's binary_logloss: 0.0138304\n",
      "[720]\tvalid_0's binary_logloss: 0.0137676\n",
      "[730]\tvalid_0's binary_logloss: 0.0137065\n",
      "[740]\tvalid_0's binary_logloss: 0.0136702\n",
      "[750]\tvalid_0's binary_logloss: 0.0136659\n",
      "[760]\tvalid_0's binary_logloss: 0.0136352\n",
      "[770]\tvalid_0's binary_logloss: 0.013604\n",
      "[780]\tvalid_0's binary_logloss: 0.0135611\n",
      "[790]\tvalid_0's binary_logloss: 0.013512\n",
      "[800]\tvalid_0's binary_logloss: 0.0134748\n",
      "[810]\tvalid_0's binary_logloss: 0.0134266\n",
      "[820]\tvalid_0's binary_logloss: 0.013393\n",
      "[830]\tvalid_0's binary_logloss: 0.0133901\n",
      "[840]\tvalid_0's binary_logloss: 0.0133468\n",
      "[850]\tvalid_0's binary_logloss: 0.0133315\n",
      "[860]\tvalid_0's binary_logloss: 0.013294\n",
      "[870]\tvalid_0's binary_logloss: 0.0132799\n",
      "[880]\tvalid_0's binary_logloss: 0.0132487\n",
      "[890]\tvalid_0's binary_logloss: 0.0132502\n",
      "[900]\tvalid_0's binary_logloss: 0.0132432\n",
      "[910]\tvalid_0's binary_logloss: 0.0132188\n",
      "[920]\tvalid_0's binary_logloss: 0.0131849\n",
      "[930]\tvalid_0's binary_logloss: 0.0132108\n",
      "[940]\tvalid_0's binary_logloss: 0.0132047\n",
      "[950]\tvalid_0's binary_logloss: 0.0131975\n",
      "[960]\tvalid_0's binary_logloss: 0.0131803\n",
      "[970]\tvalid_0's binary_logloss: 0.013177\n",
      "[980]\tvalid_0's binary_logloss: 0.0131814\n",
      "[990]\tvalid_0's binary_logloss: 0.0131869\n",
      "[1000]\tvalid_0's binary_logloss: 0.0131998\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[975]\tvalid_0's binary_logloss: 0.0131647\n"
     ]
    }
   ],
   "source": [
    "k = 5 \n",
    "models = [] \n",
    "kfold = StratifiedKFold(n_splits = k, shuffle = True, random_state = 990101) \n",
    "for n_fold, (train_idx, val_idx) in enumerate(kfold.split(x_train, y_train)): \n",
    "    train_x, val_x = x_train[train_idx], x_train[val_idx]\n",
    "    train_y, val_y = y_train[train_idx], y_train[val_idx] \n",
    "    \n",
    "    params = {'learning_rate': 0.01,\n",
    "              'max_depth': 16, \n",
    "              'objective': 'binary',\n",
    "              'metric': 'binary_logloss',\n",
    "              'is_training_metric': True,\n",
    "              'num_leaves': 128,\n",
    "              'feature_fraction': 0.9,\n",
    "              'bagging_fraction': 0.75, \n",
    "              'bagging_freq': 5,\n",
    "              'seed': 960418} \n",
    "    \n",
    "    train_ds = lgbm.Dataset(train_x, label = train_y) \n",
    "    val_ds = lgbm.Dataset(val_x, label = val_y) \n",
    "    model = lgbm.train(params, train_ds, 1000, val_ds, verbose_eval = 10, early_stopping_rounds = 100) \n",
    "    models.append(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = models[0].predict(x_test) \n",
    "pred2 = models[1].predict(x_test) \n",
    "pred3 = models[2].predict(x_test) \n",
    "pred4 = models[3].predict(x_test) \n",
    "pred5 = models[4].predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_avg = (pred1 + pred2 + pred3 + pred4 + pred5)/5.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = np.where(pred_avg > 0.5, 1, 0).reshape(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['info'] = class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('lightgbm_71_features.csv', index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
