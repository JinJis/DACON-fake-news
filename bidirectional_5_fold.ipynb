{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, Conv2DTranspose, MaxPooling2D, AveragePooling2D, BatchNormalization, concatenate, Input, ConvLSTM2D, Reshape, Conv3D, Flatten, LSTM, GRU, Dense,Dropout, Add\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Bidirectional, Conv1D, MaxPooling1D, GlobalMaxPooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re \n",
    "\n",
    "import nltk # for stopwords \n",
    "from nltk.corpus import stopwords\n",
    "import gensim # for Word2Vec embeddings \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from konlpy.tag import Mecab \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./storage/fintech_nlp/news_train.csv')\n",
    "test = pd.read_csv('./storage/fintech_nlp/news_test.csv') \n",
    "submission = pd.read_csv('./storage/fintech_nlp/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the preprocessed data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118745, 23), (118745, 61), (142565, 61), (142565, 23))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.load('./storage/fintech_nlp/y_train.npy') \n",
    "train_title = np.load('./storage/fintech_nlp/train_title_padded_x.npy')\n",
    "train_content = np.load('./storage/fintech_nlp/train_content_padded_x.npy')\n",
    "test_content = np.load('./storage/fintech_nlp/test_content_padded.npy') \n",
    "test_title = np.load('./storage/fintech_nlp/test_title_padded.npy') \n",
    "\n",
    "train_title.shape, train_content.shape, test_content.shape, test_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_content = 41573 \n",
    "vocab_title = 9197 \n",
    "embedding_vec_title = 16 \n",
    "embedding_vec_content = 64\n",
    "\n",
    "title_length = 23 \n",
    "content_length = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():  \n",
    "    input_title = Input((title_length))\n",
    "    input_content = Input((content_length)) \n",
    "    \n",
    "    emb_title = Embedding(vocab_title,embedding_vec_title)(input_title)\n",
    "    lstm_title = Bidirectional(LSTM(128, return_sequences=False))(emb_title)\n",
    "\n",
    "    emb_text = Embedding(vocab_content,embedding_vec_content)(input_content)\n",
    "    lstm_text = Bidirectional(LSTM(128, return_sequences=True))(emb_text)\n",
    "    max_pool_text = GlobalMaxPool1D()(lstm_text)\n",
    "    dropout_1_text = Dropout(0.1)(max_pool_text)\n",
    "    dense_1_text = Dense(64, activation='relu')(dropout_1_text)\n",
    "    dropout_2_text = Dropout(0.1)(dense_1_text)\n",
    "\n",
    "    out = concatenate([lstm_title,dropout_2_text],axis=-1)\n",
    "    output=Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    model = Model(inputs=[input_title, input_content], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try a k-fold ensemble instead. \n",
    "\n",
    "K = 5 seems to be a good balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Validating on Fold 1 ***\n",
      "Train on 94996 samples, validate on 23749 samples\n",
      "Epoch 1/200\n",
      "94976/94996 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9583\n",
      "Epoch 00001: val_loss improved from inf to 0.03222, saving model to ./storage/fintech_nlp_folds/kfold1/epoch_001_val_0.032.h5\n",
      "94996/94996 [==============================] - 32s 342us/sample - loss: 0.1180 - accuracy: 0.9583 - val_loss: 0.0322 - val_accuracy: 0.9889\n",
      "Epoch 2/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9929\n",
      "Epoch 00002: val_loss did not improve from 0.03222\n",
      "94996/94996 [==============================] - 24s 257us/sample - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.0372 - val_accuracy: 0.9873\n",
      "Epoch 3/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9981\n",
      "Epoch 00003: val_loss improved from 0.03222 to 0.02703, saving model to ./storage/fintech_nlp_folds/kfold1/epoch_003_val_0.027.h5\n",
      "94996/94996 [==============================] - 23s 246us/sample - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0270 - val_accuracy: 0.9915\n",
      "Epoch 4/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 00004: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 24s 250us/sample - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0360 - val_accuracy: 0.9911\n",
      "Epoch 5/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 24s 247us/sample - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0509 - val_accuracy: 0.9887\n",
      "Epoch 6/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 6.6859e-04 - accuracy: 0.9998\n",
      "Epoch 00006: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 6.6759e-04 - accuracy: 0.9998 - val_loss: 0.0461 - val_accuracy: 0.9907\n",
      "Epoch 7/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 3.0478e-04 - accuracy: 0.9999\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 24s 252us/sample - loss: 3.0432e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9906\n",
      "Epoch 8/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.3359e-04 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 1.3350e-04 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 0.9899\n",
      "Epoch 9/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.2796e-04 - accuracy: 1.0000\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 1.2776e-04 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9901\n",
      "Epoch 10/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.2342e-04 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 1.2323e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9904\n",
      "Epoch 11/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 6.0219e-05 - accuracy: 1.0000\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 6.0155e-05 - accuracy: 1.0000 - val_loss: 0.0629 - val_accuracy: 0.9894\n",
      "Epoch 12/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 5.7509e-05 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.02703\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 5.7427e-05 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9901\n",
      "*** Validating on Fold 2 ***\n",
      "Train on 94996 samples, validate on 23749 samples\n",
      "Epoch 1/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9526\n",
      "Epoch 00001: val_loss improved from inf to 0.03020, saving model to ./storage/fintech_nlp_folds/kfold2/epoch_001_val_0.030.h5\n",
      "94996/94996 [==============================] - 31s 322us/sample - loss: 0.1273 - accuracy: 0.9526 - val_loss: 0.0302 - val_accuracy: 0.9889\n",
      "Epoch 2/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9942\n",
      "Epoch 00002: val_loss improved from 0.03020 to 0.02141, saving model to ./storage/fintech_nlp_folds/kfold2/epoch_002_val_0.021.h5\n",
      "94996/94996 [==============================] - 23s 247us/sample - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.0214 - val_accuracy: 0.9925\n",
      "Epoch 3/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9982\n",
      "Epoch 00003: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0264 - val_accuracy: 0.9917\n",
      "Epoch 4/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0297 - val_accuracy: 0.9925\n",
      "Epoch 5/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 00005: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0320 - val_accuracy: 0.9926\n",
      "Epoch 6/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 5.3654e-04 - accuracy: 0.9999\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 5.3626e-04 - accuracy: 0.9999 - val_loss: 0.0360 - val_accuracy: 0.9923\n",
      "Epoch 7/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 3.7310e-04 - accuracy: 0.9999\n",
      "Epoch 00007: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 3.7254e-04 - accuracy: 0.9999 - val_loss: 0.0385 - val_accuracy: 0.9923\n",
      "Epoch 8/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 2.3113e-04 - accuracy: 0.9999\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 2.3078e-04 - accuracy: 0.9999 - val_loss: 0.0413 - val_accuracy: 0.9923\n",
      "Epoch 9/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.2203e-04 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 241us/sample - loss: 1.2185e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9922\n",
      "Epoch 10/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 9.0053e-05 - accuracy: 1.0000\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 8.9926e-05 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9921\n",
      "Epoch 11/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.1497e-04 - accuracy: 0.9999\n",
      "Epoch 00011: val_loss did not improve from 0.02141\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 1.1480e-04 - accuracy: 0.9999 - val_loss: 0.0457 - val_accuracy: 0.9922\n",
      "*** Validating on Fold 3 ***\n",
      "Train on 94996 samples, validate on 23749 samples\n",
      "Epoch 1/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9555\n",
      "Epoch 00001: val_loss improved from inf to 0.02918, saving model to ./storage/fintech_nlp_folds/kfold3/epoch_001_val_0.029.h5\n",
      "94996/94996 [==============================] - 31s 323us/sample - loss: 0.1077 - accuracy: 0.9555 - val_loss: 0.0292 - val_accuracy: 0.9889\n",
      "Epoch 2/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9955\n",
      "Epoch 00002: val_loss did not improve from 0.02918\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 0.0148 - accuracy: 0.9955 - val_loss: 0.0297 - val_accuracy: 0.9902\n",
      "Epoch 3/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 00003: val_loss improved from 0.02918 to 0.02748, saving model to ./storage/fintech_nlp_folds/kfold3/epoch_003_val_0.027.h5\n",
      "94996/94996 [==============================] - 24s 250us/sample - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0275 - val_accuracy: 0.9918\n",
      "Epoch 4/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9989\n",
      "Epoch 00004: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0361 - val_accuracy: 0.9911\n",
      "Epoch 5/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9991\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0318 - val_accuracy: 0.9913\n",
      "Epoch 6/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 00006: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0396 - val_accuracy: 0.9920\n",
      "Epoch 7/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.9850e-04 - accuracy: 1.0000\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 1.9822e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9921\n",
      "Epoch 8/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.2757e-04 - accuracy: 1.0000\n",
      "Epoch 00008: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 1.2741e-04 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9917\n",
      "Epoch 9/200\n",
      "94976/94996 [============================>.] - ETA: 0s - loss: 1.3346e-04 - accuracy: 1.0000\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 24s 258us/sample - loss: 1.3344e-04 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9915\n",
      "Epoch 10/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 9.1811e-05 - accuracy: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 9.1675e-05 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9917\n",
      "Epoch 11/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 5.6881e-05 - accuracy: 1.0000\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 24s 249us/sample - loss: 5.6795e-05 - accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9916\n",
      "Epoch 12/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 6.4687e-05 - accuracy: 1.0000\n",
      "Epoch 00012: val_loss did not improve from 0.02748\n",
      "94996/94996 [==============================] - 24s 250us/sample - loss: 6.4599e-05 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9915\n",
      "*** Validating on Fold 4 ***\n",
      "Train on 94996 samples, validate on 23749 samples\n",
      "Epoch 1/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9550\n",
      "Epoch 00001: val_loss improved from inf to 0.03618, saving model to ./storage/fintech_nlp_folds/kfold4/epoch_001_val_0.036.h5\n",
      "94996/94996 [==============================] - 31s 331us/sample - loss: 0.1110 - accuracy: 0.9551 - val_loss: 0.0362 - val_accuracy: 0.9886\n",
      "Epoch 2/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9952\n",
      "Epoch 00002: val_loss improved from 0.03618 to 0.03040, saving model to ./storage/fintech_nlp_folds/kfold4/epoch_002_val_0.030.h5\n",
      "94996/94996 [==============================] - 24s 249us/sample - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.0304 - val_accuracy: 0.9908\n",
      "Epoch 3/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9985\n",
      "Epoch 00003: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 247us/sample - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0416 - val_accuracy: 0.9900\n",
      "Epoch 4/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0409 - val_accuracy: 0.9915\n",
      "Epoch 5/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 00005: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 247us/sample - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0512 - val_accuracy: 0.9904\n",
      "Epoch 6/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 3.1809e-04 - accuracy: 0.9999\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 24s 248us/sample - loss: 3.1769e-04 - accuracy: 0.9999 - val_loss: 0.0557 - val_accuracy: 0.9909\n",
      "Epoch 7/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.5122e-04 - accuracy: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 1.5108e-04 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9917\n",
      "Epoch 8/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 7.3254e-05 - accuracy: 1.0000\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 246us/sample - loss: 7.3152e-05 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9910\n",
      "Epoch 9/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 4.5373e-05 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 4.5346e-05 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9913\n",
      "Epoch 10/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 2.3116e-05 - accuracy: 1.0000\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 2.3083e-05 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9914\n",
      "Epoch 11/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.8106e-05 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.03040\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 1.8130e-05 - accuracy: 1.0000 - val_loss: 0.0617 - val_accuracy: 0.9914\n",
      "*** Validating on Fold 5 ***\n",
      "Train on 94996 samples, validate on 23749 samples\n",
      "Epoch 1/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0972 - accuracy: 0.9596\n",
      "Epoch 00001: val_loss improved from inf to 0.02474, saving model to ./storage/fintech_nlp_folds/kfold5/epoch_001_val_0.025.h5\n",
      "94996/94996 [==============================] - 31s 330us/sample - loss: 0.0970 - accuracy: 0.9597 - val_loss: 0.0247 - val_accuracy: 0.9913\n",
      "Epoch 2/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9959\n",
      "Epoch 00002: val_loss improved from 0.02474 to 0.02397, saving model to ./storage/fintech_nlp_folds/kfold5/epoch_002_val_0.024.h5\n",
      "94996/94996 [==============================] - 23s 246us/sample - loss: 0.0132 - accuracy: 0.9959 - val_loss: 0.0240 - val_accuracy: 0.9921\n",
      "Epoch 3/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 00003: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0253 - val_accuracy: 0.9919\n",
      "Epoch 4/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0258 - val_accuracy: 0.9923\n",
      "Epoch 5/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 00005: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0295 - val_accuracy: 0.9922\n",
      "Epoch 6/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 4.7577e-04 - accuracy: 0.9999\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 243us/sample - loss: 4.7508e-04 - accuracy: 0.9999 - val_loss: 0.0305 - val_accuracy: 0.9924\n",
      "Epoch 7/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 3.0192e-04 - accuracy: 0.9999\n",
      "Epoch 00007: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 3.0146e-04 - accuracy: 0.9999 - val_loss: 0.0413 - val_accuracy: 0.9915\n",
      "Epoch 8/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 2.1003e-04 - accuracy: 1.0000\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 2.0975e-04 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9925\n",
      "Epoch 9/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 8.3943e-05 - accuracy: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 245us/sample - loss: 1.3026e-04 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 0.9928\n",
      "Epoch 10/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 1.9498e-04 - accuracy: 1.0000\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 242us/sample - loss: 1.9471e-04 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 0.9921\n",
      "Epoch 11/200\n",
      "94848/94996 [============================>.] - ETA: 0s - loss: 8.7863e-05 - accuracy: 1.0000\n",
      "Epoch 00011: val_loss did not improve from 0.02397\n",
      "94996/94996 [==============================] - 23s 244us/sample - loss: 8.7734e-05 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "def k_fold(k, files): \n",
    "    folds = [] \n",
    "    fold_size = len(files)//k \n",
    "    for i in range(k): \n",
    "        if i == k-1:  \n",
    "            l = files[i*fold_size:]\n",
    "        else: \n",
    "            l = files[i*fold_size:(i+1)*fold_size] \n",
    "        folds.append(l) \n",
    "    return folds\n",
    "\n",
    "train_title, train_content, y_train = shuffle(train_title, train_content, y_train)\n",
    "K = 5 \n",
    "train_title_folds = k_fold(K, train_title) \n",
    "train_content_folds = k_fold(K, train_content)\n",
    "train_y_folds = k_fold(K, y_train)\n",
    "\n",
    "# conduct training \n",
    "for t in range(K):\n",
    "    print(\"*** Validating on Fold {} ***\".format(t+1))\n",
    "    cur_val_title = train_title_folds[t] \n",
    "    cur_val_content = train_content_folds[t] \n",
    "    cur_val_y = train_y_folds[t] \n",
    "    \n",
    "    cur_train_title_folds = train_title_folds[0:t] + train_title_folds[t+1:] \n",
    "    cur_train_content_folds = train_content_folds[0:t] + train_content_folds[t+1:] \n",
    "    cur_train_y_folds = train_y_folds[0:t] + train_y_folds[t+1:] \n",
    "    \n",
    "    cur_train_title = [] \n",
    "    for fold in cur_train_title_folds: \n",
    "        for data in fold: \n",
    "            cur_train_title.append(data) \n",
    "    cur_train_title = np.asarray(cur_train_title) \n",
    "    \n",
    "    cur_train_content = []\n",
    "    for fold in cur_train_content_folds: \n",
    "        for data in fold: \n",
    "            cur_train_content.append(data) \n",
    "    cur_train_content = np.asarray(cur_train_content) \n",
    "    \n",
    "    cur_train_y = [] \n",
    "    for fold in cur_train_y_folds: \n",
    "        for data in fold:  \n",
    "            cur_train_y.append(data) \n",
    "    cur_train_y = np.asarray(cur_train_y) \n",
    "    \n",
    "    model_path = './storage/fintech_nlp_folds/kfold' + str(t+1) + '/epoch_{epoch:03d}_val_{val_loss:.3f}.h5' \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, verbose = 1, factor = 0.5)\n",
    "    checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 9) \n",
    "    \n",
    "    model = build_model() \n",
    "    history = model.fit(x=[cur_train_title, cur_train_content],\n",
    "                        y=cur_train_y,\n",
    "                        batch_size=128,\n",
    "                        epochs=200,\n",
    "                        validation_data=([cur_val_title, cur_val_content],cur_val_y),\n",
    "                        verbose = 1,\n",
    "                        callbacks = [learning_rate_reduction, checkpoint, early_stopping]) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the prediction using the trained model and generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('./storage/fintech_nlp_folds/kfold1/epoch_003_val_0.027.h5')\n",
    "model2 = load_model('./storage/fintech_nlp_folds/kfold2/epoch_002_val_0.021.h5')\n",
    "model3 = load_model('./storage/fintech_nlp_folds/kfold3/epoch_003_val_0.027.h5')\n",
    "model4 = load_model('./storage/fintech_nlp_folds/kfold4/epoch_002_val_0.030.h5') \n",
    "model5 = load_model('./storage/fintech_nlp_folds/kfold5/epoch_002_val_0.024.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict([test_title,test_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = model2.predict([test_title,test_content]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = model3.predict([test_title,test_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = model4.predict([test_title,test_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred5 = model5.predict([test_title,test_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (pred1 + pred2 + pred3 + pred4 + pred5)/5.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = np.where(predictions > 0.5, 1,0).reshape(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00649852e-01],\n",
       "       [1.08420845e-05],\n",
       "       [2.54368788e-04],\n",
       "       ...,\n",
       "       [9.99999225e-01],\n",
       "       [9.99964595e-01],\n",
       "       [9.99964595e-01]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWS00237_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEWS00237_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWS00237_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWS00237_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWS00237_5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  info\n",
       "0  NEWS00237_1     0\n",
       "1  NEWS00237_2     0\n",
       "2  NEWS00237_3     0\n",
       "3  NEWS00237_4     0\n",
       "4  NEWS00237_5     0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['info'] = class_pred \n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./storage/bidirectional_lstm_5fold.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
